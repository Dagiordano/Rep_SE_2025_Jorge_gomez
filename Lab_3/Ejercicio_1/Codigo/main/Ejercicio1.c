#include <stdio.h>
#include <math.h>
#include <esp_dsp.h>
#include <esp_timer.h>
#include <string.h>

// Network architecture: 1 -> 16 -> 16 -> 1
#define INPUT_SIZE 1
#define HIDDEN1_SIZE 16
#define HIDDEN2_SIZE 16
#define OUTPUT_SIZE 1

// Network weights and biases
// Layer 1: 1 input -> 16 neurons
float weights_layer1[INPUT_SIZE * HIDDEN1_SIZE] = {
    -0.12180936336517334f,
    -0.3929499089717865f,
    0.1962481290102005f,
    0.6956225633621216f,
    0.14335781335830688f,
    -0.06439566612243652f,
    -0.18021363019943237f,
    -0.054397642612457275f,
    -0.4619080424308777f,
    0.4055696427822113f,
    0.27962690591812134f,
    -0.5141448378562927f,
    -0.42096662521362305f,
    0.6256062984466553f,
    -0.37430816888809204f,
    -0.33132314682006836f
};
float bias_layer1[HIDDEN1_SIZE] = {
    0.0f,
    0.0f,
    0.3786514103412628f,
    -0.7221674919128418f,
    0.09094736725091934f,
    0.8576537370681763f,
    0.0f,
    0.0f,
    0.0f,
    1.1846195459365845f,
    -0.20879396796226501f,
    0.0f,
    0.0f,
    -0.011825152672827244f,
    0.0f,
    0.0f
};

// Layer 2: 16 -> 16 neurons
float weights_layer2[HIDDEN1_SIZE * HIDDEN2_SIZE] = {
    // Row 0
    0.0027225911617279053f, 0.18983474373817444f, 0.41336789727211f, -0.18240013718605042f, 0.2007804811000824f, -0.19718044996261597f, -0.06138917803764343f, -0.12958911061286926f, -0.12484398484230042f, -0.4296090304851532f, -0.3490271270275116f, 0.3468421995639801f, 0.3684578835964203f, 0.18269559741020203f, 0.23875799775123596f, -0.2323986440896988f,
    // Row 1
    -0.025780469179153442f, 0.19623693823814392f, 0.03369263559579849f, -0.6074578166007996f, 0.07136324793100357f, 0.401083379983902f, -0.39895179867744446f, -0.12483623623847961f, -0.3789287507534027f, 0.2639991343021393f, -0.3698916733264923f, 0.34108874201774597f, -0.26812106370925903f, 0.16135936975479126f, -0.1342160403728485f, 0.36499324440956116f,
    // Row 2
    0.14257463812828064f, -0.18008947372436523f, 0.35641762614250183f, -0.5778324604034424f, -0.393827348947525f, 0.8056795001029968f, -0.263570100069046f, -0.01627323031425476f, 0.3025842607021332f, 0.4566139876842499f, 0.21970704197883606f, -0.21923679113388062f, 0.15798768401145935f, -0.5030084848403931f, 0.33370068669319153f, -0.14506402611732483f,
    // Row 3
    -0.2073986679315567f, -0.35889649391174316f, 0.1797211617231369f, 0.4593379497528076f, 0.20270542800426483f, -0.39704400300979614f, 0.3732830584049225f, -0.3063697814941406f, 0.3175797164440155f, -0.1850675493478775f, 0.2977258861064911f, -0.2707940936088562f, -0.2222711741924286f, -0.1752738207578659f, -0.23541268706321716f, -0.03539720177650452f,
    // Row 4
    0.07093545794487f, -0.2856180667877197f, -0.1195007860660553f, 0.37393417954444885f, -0.032085925340652466f, -0.42951568961143494f, 0.3786592185497284f, 0.06391224265098572f, 0.10423329472541809f, -0.007706940174102783f, 0.14500054717063904f, 0.039341628551483154f, 0.03364381194114685f, -0.3499203324317932f, -0.13348707556724548f, -0.3816155195236206f,
    // Row 5
    0.003396540880203247f, 0.11320623755455017f, 0.02002820558845997f, 0.11572672426700592f, -0.005094918888062239f, 0.42318350076675415f, -0.0019834041595458984f, 0.27106973528862f, -0.07390880584716797f, -0.18350361287593842f, -0.09491398930549622f, -0.060689955949783325f, 0.23715534806251526f, 0.2811555862426758f, -0.04390755295753479f, 0.14753779768943787f,
    // Row 6
    -0.3940480947494507f, -0.29011958837509155f, -0.02212396264076233f, 0.255339115858078f, -0.3506515622138977f, -0.4268346130847931f, 0.2926781475543976f, 0.04957926273345947f, 0.25725534558296204f, -0.3673473596572876f, 0.09449979662895203f, 0.3963540494441986f, -0.3343580961227417f, -0.07587239146232605f, -0.342684268951416f, 0.4181382358074188f,
    // Row 7
    -0.022124379873275757f, 0.0018830597400665283f, 0.12194328010082245f, 0.5240871906280518f, -0.39801761507987976f, -0.43094855546951294f, -0.3179985582828522f, -0.24505843222141266f, -0.20615866780281067f, -0.23297451436519623f, 0.37499183416366577f, 0.01292884349822998f, 0.012023955583572388f, -0.20294255018234253f, -0.10908496379852295f, 0.13056108355522156f,
    // Row 8
    0.11879357695579529f, 0.20404520630836487f, -0.16572046279907227f, 0.1449335813522339f, 0.12137463688850403f, 0.07978656888008118f, -0.29415494203567505f, -0.09883362054824829f, 0.08394333720207214f, 0.2731185555458069f, -0.04877723380923271f, -0.40059465169906616f, 0.4029903709888458f, 0.3822925090789795f, -0.42965972423553467f, 0.01964583992958069f,
    // Row 9
    0.02541375160217285f, -0.3499251902103424f, -0.02137753553688526f, 0.4022998809814453f, -0.19355754554271698f, -0.45235371589660645f, -0.30786943435668945f, 0.19118043780326843f, -0.21551164984703064f, -0.5623782277107239f, 0.40723878145217896f, 0.006060183048248291f, -0.21781985461711884f, 0.37455931305885315f, -0.062002331018447876f, -0.27808982133865356f,
    // Row 10
    0.028215527534484863f, 0.25770387053489685f, 0.18670712411403656f, -0.4259430468082428f, -0.005115911364555359f, 0.8945462703704834f, 0.3702852427959442f, 0.2569825351238251f, 0.04572060704231262f, -0.02441404201090336f, -0.06804883480072021f, 0.2841321527957916f, -0.14558589458465576f, -0.6402272582054138f, 0.3264928162097931f, 0.4267444908618927f,
    // Row 11
    -0.24531064927577972f, 0.3835720121860504f, 0.16226670145988464f, -0.11394450068473816f, 0.053048670291900635f, -0.015474289655685425f, 0.33776620030403137f, -0.03666755557060242f, -0.06950125098228455f, -0.3659064769744873f, -0.08370888233184814f, 0.25855931639671326f, 0.057923585176467896f, 0.4006575047969818f, 0.007971644401550293f, -0.3471950590610504f,
    // Row 12
    -0.3998015522956848f, -0.22428658604621887f, 0.2061450332403183f, -0.13268886506557465f, 0.2491515874862671f, 0.8166562914848328f, 0.12804529070854187f, -0.30007338523864746f, -0.15637004375457764f, 0.47431984543800354f, -0.19108781218528748f, 0.33162859082221985f, 0.05137836933135986f, -0.3956833481788635f, -0.10591194033622742f, 0.34808632731437683f,
    // Row 13
    0.36735162138938904f, 0.3183262050151825f, -0.24144241213798523f, -0.2489166259765625f, -0.24684159457683563f, 0.7941295504570007f, 0.4072605073451996f, 0.013025254011154175f, -0.32647430896759033f, -0.11593519896268845f, 0.5575068593025208f, 0.01566949486732483f, 0.1340005099773407f, -0.6853724122047424f, 0.08650955557823181f, 0.2571868598461151f,
    // Row 14
    -0.09131631255149841f, -0.08977609872817993f, -0.15830881893634796f, 0.2629053592681885f, 0.18760274350643158f, -0.4070013165473938f, 0.11684796214103699f, -0.429512619972229f, -0.268528014421463f, 0.24691413342952728f, -0.14726854860782623f, 0.26559004187583923f, -0.28075987100601196f, 0.25861656665802f, -0.41818663477897644f, -0.08954474329948425f,
    // Row 15
    0.21914830803871155f, 0.2879165709018707f, -0.26394495368003845f, -0.32984206080436707f, 0.1691901683807373f, 0.042184554040431976f, -0.20519524812698364f, -0.3780844509601593f, -0.004826903343200684f, 0.33323872089385986f, 0.30983003973960876f, -0.1788569986820221f, -0.1724168062210083f, 0.057067230343818665f, -0.31484460830688477f, -0.3968038260936737f
};
float bias_layer2[HIDDEN2_SIZE] = {
    0.0f,
    0.5027430653572083f,
    0.4076739549636841f,
    -0.2554803490638733f,
    0.0f,
    0.1749088615179062f,
    0.0f,
    -0.24164630472660065f,
    0.07021674513816833f,
    -0.43093863129615784f,
    0.597221851348877f,
    0.0f,
    0.2202627807855606f,
    0.5447701215744019f,
    -0.19466280937194824f,
    -0.07130637764930725f
};

// Output layer: 16 -> 1 neuron
float weights_output[HIDDEN2_SIZE * OUTPUT_SIZE] = {
    -0.47172480821609497f,
    1.2040079832077026f,
    1.2589000463485718f,
    0.358551561832428f,
    0.11061286926269531f,
    -0.2626565396785736f,
    0.33092570304870605f,
    0.8805835247039795f,
    -0.18485286831855774f,
    1.0813982486724854f,
    -0.7373619675636292f,
    0.31201863288879395f,
    -0.850340723991394f,
    -0.9309349656105042f,
    0.13627851009368896f,
    0.11121345311403275f
};
float bias_output[OUTPUT_SIZE] = {-0.16013234853744507f};

// Intermediate buffers
float hidden1_output[HIDDEN1_SIZE];
float hidden2_output[HIDDEN2_SIZE];
float final_output[OUTPUT_SIZE];

// ReLU activation function using DSP operations
void relu_activation(float* data, int size) {
    for (int i = 0; i < size; i++) {
        if (data[i] < 0.0f) {
            data[i] = 0.0f;
        }
    }
}

// Dense layer forward pass using ESP-DSP matrix multiplication
void dense_layer_forward(const float* input, const float* weights, const float* bias, 
                        float* output, int input_size, int output_size) {
    // Initialize output with bias
    memcpy(output, bias, output_size * sizeof(float));
    
    // Matrix-vector multiplication: output += weights * input
    // weights is stored in row-major format (output_size x input_size)
    for (int i = 0; i < output_size; i++) {
        for (int j = 0; j < input_size; j++) {
            output[i] += weights[i * input_size + j] * input[j];
        }
    }
}

// Neural network inference
float neural_network_inference(float input_value) {
    float input[INPUT_SIZE] = {input_value};
    
    // Layer 1: Input -> Hidden1 (with ReLU)
    dense_layer_forward(input, weights_layer1, bias_layer1, hidden1_output, INPUT_SIZE, HIDDEN1_SIZE);
    relu_activation(hidden1_output, HIDDEN1_SIZE);
    
    // Layer 2: Hidden1 -> Hidden2 (with ReLU)
    dense_layer_forward(hidden1_output, weights_layer2, bias_layer2, hidden2_output, HIDDEN1_SIZE, HIDDEN2_SIZE);
    relu_activation(hidden2_output, HIDDEN2_SIZE);
    
    // Output layer: Hidden2 -> Output (no activation)
    dense_layer_forward(hidden2_output, weights_output, bias_output, final_output, HIDDEN2_SIZE, OUTPUT_SIZE);
    
    return final_output[0];
}

void app_main(void)
{
    printf("Neural Network Inference Test\n");
    printf("Architecture: %d -> %d -> %d -> %d\n", INPUT_SIZE, HIDDEN1_SIZE, HIDDEN2_SIZE, OUTPUT_SIZE);
    printf("================================\n");
    
    // Initialize DSP library
    esp_err_t ret = dsps_fft2r_init_fc32(NULL, CONFIG_DSP_MAX_FFT_SIZE);
    if (ret != ESP_OK) {
        printf("DSP initialization failed: %s\n", esp_err_to_name(ret));
        return;
    }
    
    // Test inputs
    float test_inputs[] = {0.0f, 0.5f, 1.0f, -0.5f, -1.0f, 2.0f, -2.0f, 0.75f, -0.25f, 1.5f};
    int num_tests = sizeof(test_inputs) / sizeof(test_inputs[0]);
    
    printf("\nRunning inference tests:\n");
    printf("Input\t\tOutput\t\tTime (μs)\n");
    printf("-----\t\t------\t\t---------\n");
    
    for (int i = 0; i < num_tests; i++) {
        // Time the inference
        int64_t start_time = esp_timer_get_time();
        float result = neural_network_inference(test_inputs[i]);
        int64_t end_time = esp_timer_get_time();
        
        int64_t inference_time = end_time - start_time;
        
        printf("%.2f\t\t%.6f\t\t%lld\n", test_inputs[i], result, inference_time);
    }
    
    // Performance benchmark with multiple iterations
    printf("\nPerformance benchmark (1000 iterations):\n");
    float benchmark_input = 0.5f;
    int num_iterations = 1000;
    
    int64_t total_start = esp_timer_get_time();
    for (int i = 0; i < num_iterations; i++) {
        neural_network_inference(benchmark_input);
    }
    int64_t total_end = esp_timer_get_time();
    
    int64_t total_time = total_end - total_start;
    float avg_time = (float)total_time / num_iterations;
    
    printf("Total time for %d inferences: %lld μs\n", num_iterations, total_time);
    printf("Average time per inference: %.2f μs\n", avg_time);
    printf("Inferences per second: %.0f\n", 1000000.0f / avg_time);
    
    printf("\nNeural network inference complete!\n");
}
